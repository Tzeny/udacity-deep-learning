{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_word2vec.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D7tqLMoKF6uq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 5\n",
        "------------\n",
        "\n",
        "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
      ]
    },
    {
      "metadata": {
        "id": "0K1ZyLn04QZf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCjPJE944bkV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the data from the source website if necessary."
      ]
    },
    {
      "metadata": {
        "id": "RJ-o3UBUFtCw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "both",
        "outputId": "ba3d2ccd-2ba5-46f8-a5f5-48b2196852d2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216594519,
          "user_tz": -180,
          "elapsed": 46647,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "  if not os.path.exists(filename):\n",
        "    filename, _ = urlretrieve(url + filename, filename)\n",
        "  statinfo = os.stat(filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and verified %s' % filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception(\n",
        "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "  return filename\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zqz3XiqI4mZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the data into a string."
      ]
    },
    {
      "metadata": {
        "id": "Mvf09fjugFU_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "both",
        "outputId": "4b157fdc-3e66-4373-b1fa-665309082489",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216597791,
          "user_tz": -180,
          "elapsed": 3109,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "  return data\n",
        "  \n",
        "words = read_data(filename)\n",
        "print('Data size %d' % len(words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zdw6i4F8glpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build the dictionary and replace rare words with UNK token."
      ]
    },
    {
      "metadata": {
        "id": "gAL1EECXeZsD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "cellView": "both",
        "outputId": "403d4ebd-13fa-488d-cb64-d1da6448298c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216608348,
          "user_tz": -180,
          "elapsed": 10409,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = 200000\n",
        "\n",
        "def build_dataset(words):\n",
        "  count = [['UNK', -1]]\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
        "  dictionary = dict()\n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary)\n",
        "  data = list()\n",
        "  unk_count = 0\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0  # dictionary['UNK']\n",
        "      unk_count = unk_count + 1\n",
        "    data.append(index)\n",
        "  count[0][1] = unk_count\n",
        "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
        "  return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10])\n",
        "del words  # Hint to reduce memory."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 53855], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
            "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFwoyygOmWsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to generate a training batch for the skip-gram model."
      ]
    },
    {
      "metadata": {
        "id": "w9APjA-zmfjV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "cellView": "both",
        "outputId": "64e30010-1fcf-45ef-8908-a4dae0a9ba3e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216609598,
          "user_tz": -180,
          "elapsed": 1121,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_index = 0\n",
        "\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "  global data_index\n",
        "  assert batch_size % num_skips == 0\n",
        "    \n",
        "  assert num_skips <= 2 * skip_window\n",
        "\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "\n",
        "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  for i in range(batch_size // num_skips):\n",
        "    target = skip_window  # target label at the center of the buffer\n",
        "    targets_to_avoid = [ skip_window ] #make sure we don't pick the word itself as a label\n",
        "    for j in range(num_skips): #num_skips = the number of times we want to use the same input to generate a label\n",
        "      #make sure we don't pick the same label twice for a word\n",
        "      while target in targets_to_avoid:\n",
        "        target = random.randint(0, span - 1)\n",
        "      targets_to_avoid.append(target)\n",
        "      batch[i * num_skips + j] = buffer[skip_window]\n",
        "      labels[i * num_skips + j, 0] = buffer[target]\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  return batch, labels\n",
        "\n",
        "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
        "\n",
        "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
        "    data_index = 0\n",
        "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
        "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
        "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
        "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
            "\n",
            "with num_skips = 2 and skip_window = 1:\n",
            "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
            "    labels: ['as', 'anarchism', 'originated', 'a', 'as', 'term', 'a', 'of']\n",
            "\n",
            "with num_skips = 4 and skip_window = 2:\n",
            "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
            "    labels: ['term', 'a', 'originated', 'anarchism', 'originated', 'as', 'of', 'term']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ofd1MbBuwiva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train a skip-gram model."
      ]
    },
    {
      "metadata": {
        "id": "8pQKsV4Vwlzy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "cellView": "both",
        "outputId": "7d04b52b-aa8d-4a68-a6f5-fd23d77ed018",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216611155,
          "user_tz": -180,
          "elapsed": 1429,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "skip_window = 1 # How many words to consider left and right.\n",
        "num_skips = 2 # How many times to reuse an input to generate a label.\n",
        "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent. \n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
        "num_sampled = 64 # Number of negative examples to sample.\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data.\n",
        "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "    # Variables.\n",
        "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    softmax_weights = tf.Variable(\n",
        "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "    # Model.\n",
        "    # Look up embeddings for inputs.\n",
        "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
        "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "    loss = tf.reduce_mean(\n",
        "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
        "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "\n",
        "    # Optimizer.\n",
        "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "    # This is because the embeddings are defined as a variable quantity and the\n",
        "    # optimizer's `minimize` method will by default modify all variable quantities \n",
        "    # that contribute to the tensor it is passed.\n",
        "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
        "    optimizer = tf.train.AdagradOptimizer(1).minimize(loss)\n",
        "\n",
        "    # Compute the similarity between minibatch examples and all embeddings.\n",
        "    # We use the cosine distance:\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "    normalized_embeddings = embeddings / norm\n",
        "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1bQFGceBxrWW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 100001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Initialized')\n",
        "    average_loss = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
        "        \n",
        "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
        "        \n",
        "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "        \n",
        "        average_loss += l\n",
        "        if step % 2000 == 0:\n",
        "            if step > 0:\n",
        "                average_loss = average_loss / 2000\n",
        "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "            print('Average loss at step %d: %f' % (step, average_loss))\n",
        "            average_loss = 0\n",
        "            \n",
        "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "        if step % 10000 == 0:\n",
        "            sim = similarity.eval()\n",
        "            for i in range(valid_size):\n",
        "                valid_word = reverse_dictionary[valid_examples[i]]\n",
        "                top_k = 8 # number of nearest neighbors\n",
        "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
        "                log = 'Nearest to %s:' % valid_word\n",
        "                for k in range(top_k):\n",
        "                    close_word = reverse_dictionary[nearest[k]]\n",
        "                    log = '%s %s,' % (log, close_word)\n",
        "                print(log)\n",
        "    final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p3BBwFO6cy_Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "e9c2a24b-aacb-472c-9589-6122e95852d2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529219133142,
          "user_tz": -180,
          "elapsed": 3984,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_embed = final_embeddings\n",
        "word = 'man'\n",
        "queen = test_embed[dictionary[word]]\n",
        "\n",
        "print(test_embed.shape)\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  \n",
        "  similarity = tf.matmul(queen.reshape(1,128), tf.transpose(test_embed))\n",
        " \n",
        "  similarity_eval = similarity.eval()\n",
        "  \n",
        "  top_k = 8 # number of nearest neighbors\n",
        "  nearest = (-similarity_eval[0, :]).argsort()[1:top_k+1]\n",
        "  print(nearest)\n",
        "  log = 'Nearest to '+word+':'\n",
        "  for k in range(top_k):\n",
        "      close_word = reverse_dictionary[nearest[k]]\n",
        "      log = '%s %s,' % (log, close_word)\n",
        "  print(log)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 39152, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 696, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 775, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[   398   1013   1498 127765   7917 146152  13318   6098]\n",
            "Nearest to man: person, woman, boy, chanute, glasgow, programmatical, trudeau, revenge,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "74bhgBtWjR4E",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "303287b7-806d-486a-83a9-2b65cdec2ac5",
        "executionInfo": {
          "status": "error",
          "timestamp": 1529219074376,
          "user_tz": -180,
          "elapsed": 65849,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('skip-gram-embeddings-200000v-128e.pickle', 'wb') as handle:\n",
        "    pickle.dump(final_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "from google.colab import files\n",
        "files.download('skip-gram-embeddings-200000v-128e.pickle')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-db4c585efa75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip-gram-embeddings-200000v-128e.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jjJXYA_XzV79",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "num_points = 400\n",
        "\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
        "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_e0D_UezcDe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "def plot(embeddings, labels):\n",
        "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
        "  pylab.figure(figsize=(15,15))  # in inches\n",
        "  for i, label in enumerate(labels):\n",
        "    x, y = embeddings[i,:]\n",
        "    pylab.scatter(x, y)\n",
        "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
        "                   ha='right', va='bottom')\n",
        "  pylab.show()\n",
        "\n",
        "words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
        "plot(two_d_embeddings, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QB5EFrBnpNnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Problem\n",
        "-------\n",
        "\n",
        "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "Ff0bYNEPmQFI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "943b3505-89a0-45b9-9e94-093208c1ff94",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529187067961,
          "user_tz": -180,
          "elapsed": 847,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_index = 0\n",
        "\n",
        "def generate_batch_cow(batch_size, R):\n",
        "    global data_index\n",
        "    buffer_index = 0\n",
        "    \n",
        "    assert R > 0\n",
        "\n",
        "    batch = np.ndarray(shape=(batch_size, R*2), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    \n",
        "    data_len = batch_size + 4\n",
        "    buffer = collections.deque(maxlen=data_len)\n",
        "    \n",
        "    #fill our buffer with enough entries from the data input\n",
        "    for _ in range(data_len):\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "        \n",
        "    for i in range(batch_size):\n",
        "        target = R  # target label at the center of the buffer\n",
        "        for j in range(2*R+1):\n",
        "            if j==target:#skip over the middle word\n",
        "                continue\n",
        "            if j<target:#set first R words\n",
        "                batch[i][j] = buffer[i + j]\n",
        "            else:#the next R words will be offset by 1 in the vector\n",
        "                batch[i][j-1] = buffer[i + j]\n",
        "        labels[i] = buffer[i+target];\n",
        "    return batch, labels\n",
        "\n",
        "print('data encoded: ', data[:16])\n",
        "print('data:', [reverse_dictionary[di] for di in data[:16]])\n",
        "\n",
        "for R in range(1,3):\n",
        "    data_index = 0\n",
        "    batch, labels = generate_batch_cow(batch_size=8, R=R)\n",
        "    print('\\nwith R= %d: ' % (R))\n",
        "    for j in range(0,batch.shape[0]):\n",
        "        print('    batch ',j,': ', [reverse_dictionary[bi] for bi in batch[:][j]])\n",
        "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data encoded:  [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156, 128, 742, 477, 10572, 134, 1]\n",
            "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the']\n",
            "\n",
            "with R= 1: \n",
            "    batch  0 :  ['anarchism', 'as']\n",
            "    batch  1 :  ['originated', 'a']\n",
            "    batch  2 :  ['as', 'term']\n",
            "    batch  3 :  ['a', 'of']\n",
            "    batch  4 :  ['term', 'abuse']\n",
            "    batch  5 :  ['of', 'first']\n",
            "    batch  6 :  ['abuse', 'used']\n",
            "    batch  7 :  ['first', 'against']\n",
            "    labels: ['originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used']\n",
            "\n",
            "with R= 2: \n",
            "    batch  0 :  ['anarchism', 'originated', 'a', 'term']\n",
            "    batch  1 :  ['originated', 'as', 'term', 'of']\n",
            "    batch  2 :  ['as', 'a', 'of', 'abuse']\n",
            "    batch  3 :  ['a', 'term', 'abuse', 'first']\n",
            "    batch  4 :  ['term', 'of', 'first', 'used']\n",
            "    batch  5 :  ['of', 'abuse', 'used', 'against']\n",
            "    batch  6 :  ['abuse', 'first', 'against', 'early']\n",
            "    batch  7 :  ['first', 'used', 'early', 'working']\n",
            "    labels: ['as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t6QuAahamQFN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "c19a7781-ab7f-4b0e-ec71-0ae3c049ff91",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529187074959,
          "user_tz": -180,
          "elapsed": 1919,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "cbow_r = 2#how many words in the past and future do we retain\n",
        "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent. \n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.ndarray(shape=(valid_size, cbow_r*2), dtype=np.int32)\n",
        "valid_labels = np.empty(shape=(valid_size), dtype=np.int32)\n",
        "num_sampled = 64 # Number of negative examples to sample.\n",
        "\n",
        "\n",
        "for key,i in enumerate(random.sample(range(valid_window), valid_size)):\n",
        "    target = cbow_r  # target label at the center of the buffer\n",
        "    for j in range(2*cbow_r+1):\n",
        "        if j==target:#skip over the middle word\n",
        "            valid_labels[key] = data[i + j]\n",
        "            continue\n",
        "        if j<target:#set first R words\n",
        "            valid_examples[key][j] = data[i + j]\n",
        "        else:#the next R words will be offset by 1 in the vector\n",
        "            valid_examples[key][j-1] = data[i + j]\n",
        "   \n",
        "#print(valid_examples)\n",
        "#print(valid_labels)\n",
        "with tf.device('/gpu:0'):\n",
        "  graph = tf.Graph()\n",
        "\n",
        "  with graph.as_default():\n",
        "\n",
        "      # Input data.\n",
        "      train_dataset = tf.placeholder(tf.int32, shape=[batch_size,cbow_r*2])\n",
        "      train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "      valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "      # Variables.\n",
        "      embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "      softmax_weights = tf.Variable(\n",
        "          tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                               stddev=1.0 / math.sqrt(embedding_size)))\n",
        "      softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "      global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
        "      decay_learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.96)\n",
        "\n",
        "      # Model.\n",
        "      # Look up embeddings for inputs.\n",
        "     #train_dataset_reshaped = tf.reshape(train_dataset,(batch_size,embedding_size * cbow_r))\n",
        "      #embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
        "      #embed = tf.reshape(embed, (batch_size,-1))\n",
        "      embed = None\n",
        "      for i in range(2*cbow_r):\n",
        "          embed_batch = tf.nn.embedding_lookup( embeddings , train_dataset[:,i])\n",
        "          emb_x,emb_y = embed_batch.get_shape().as_list()\n",
        "\n",
        "          if embed is None:\n",
        "              print('embed is None, hence reshaping  from %s to  (%s,%s)' %(embed_batch.get_shape(), emb_x, emb_y) )\n",
        "              embed = tf.reshape(embed_batch, [emb_x, emb_y, 1])\n",
        "          else:\n",
        "              print('embed is not None, hence  concating earlier embed %s with current embed_batch'\\\n",
        "                        %(embed.get_shape()) )\n",
        "              embed = tf.concat([embed,tf.reshape(embed_batch,[emb_x,emb_y,1])],2)\n",
        "\n",
        "      embed =  tf.reduce_mean(embed,2,keepdims=False)\n",
        "\n",
        "      # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "      loss = tf.reduce_mean(\n",
        "      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
        "                                 labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "\n",
        "      # Optimizer.\n",
        "      # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "      # This is because the embeddings are defined as a variable quantity and the\n",
        "      # optimizer's `minimize` method will by default modify all variable quantities \n",
        "      # that contribute to the tensor it is passed.\n",
        "      # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
        "      optimizer = tf.train.AdamOptimizer(0.001)\n",
        "\n",
        "      train_op = optimizer.minimize(loss, global_step = global_step)\n",
        "\n",
        "      # Compute the similarity between minibatch examples and all embeddings.\n",
        "      # We use the cosine distance:\n",
        "      norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "      normalized_embeddings = embeddings / norm\n",
        "\n",
        "      #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "      #valid_embeddings = tf.reshape(valid_embeddings, (valid_size,-1))\n",
        "\n",
        "      valid_embeddings = None\n",
        "      for i in range(2*cbow_r):\n",
        "          valid_embeddings_batch = tf.nn.embedding_lookup( embeddings , valid_dataset[:,i])\n",
        "          emb_x,emb_y = valid_embeddings_batch.get_shape().as_list()\n",
        "\n",
        "          if valid_embeddings is None:\n",
        "              print('valid_embeddings is None, hence reshaping  from %s to  (%s,%s)' \n",
        "                    % (valid_embeddings_batch.get_shape(), emb_x, emb_y) )\n",
        "              valid_embeddings = tf.reshape(valid_embeddings_batch, [emb_x, emb_y, 1])\n",
        "          else:\n",
        "              print('valid_embeddings is not None, hence  concating earlier embed %s with current embed_batch'\\\n",
        "                        %(valid_embeddings.get_shape()) )\n",
        "              valid_embeddings = tf.concat([valid_embeddings,tf.reshape(valid_embeddings_batch,[emb_x,emb_y,1])],2)\n",
        "\n",
        "      valid_embeddings =  tf.reduce_mean(valid_embeddings,2,keepdims=False)\n",
        "\n",
        "      predictions = tf.nn.softmax(tf.matmul(valid_embeddings, tf.transpose(softmax_weights)) + softmax_biases)\n",
        "      #logits = tf.matmul(valid_embeddings, tf.transpose(softmax_weights))\n",
        "      #logits = tf.nn.bias_add(logits, softmax_biases)\n",
        "\n",
        "\n",
        "      #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "      #similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed is None, hence reshaping  from (128, 128) to  (128,128)\n",
            "embed is not None, hence  concating earlier embed (128, 128, 1) with current embed_batch\n",
            "embed is not None, hence  concating earlier embed (128, 128, 2) with current embed_batch\n",
            "embed is not None, hence  concating earlier embed (128, 128, 3) with current embed_batch\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n",
            "valid_embeddings is None, hence reshaping  from (16, 128) to  (16,128)\n",
            "valid_embeddings is not None, hence  concating earlier embed (16, 128, 1) with current embed_batch\n",
            "valid_embeddings is not None, hence  concating earlier embed (16, 128, 2) with current embed_batch\n",
            "valid_embeddings is not None, hence  concating earlier embed (16, 128, 3) with current embed_batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x1I67dyomQFY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "65ecb65b-a8de-4983-88d6-71495513361d"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph) as session:\n",
        "    params = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
        "    ids = tf.constant([0,2,3,4])\n",
        "    embed = tf.nn.embedding_lookup(params,ids).eval()\n",
        "    print(embed[0][0]+embed[1][0]+embed[2][0]+embed[3][0])\n",
        "    embed = tf.reduce_mean(embed, 0, keepdims=False)\n",
        "    print(embed)\n",
        "    print(embed.shape)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8401964\n",
            "Tensor(\"Mean_19:0\", shape=(128,), dtype=float32)\n",
            "(128,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZnHSRupYocMs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "f7565e83-f5c6-48da-fc3b-c3e97b98c3cd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529216523496,
          "user_tz": -180,
          "elapsed": 10320,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.3)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.5)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "[<GPUtil.GPUtil.GPU object at 0x7fdab7ae3ba8>]\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 147.2 MB\n",
            "GPU RAM Free: 11439MB | Used: 0MB | Util   0% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMpSk5OMpAlf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rCnJJrCSmQFh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3977
        },
        "outputId": "6526a47e-e374-4097-c63f-4d2d14a11e67",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529190453639,
          "user_tz": -180,
          "elapsed": 3360980,
          "user": {
            "displayName": "Andrei Tenescu",
            "photoUrl": "//lh6.googleusercontent.com/-lMa5HRoF0W8/AAAAAAAAAAI/AAAAAAAABgE/8v1PeBmW2pk/s50-c-k-no/photo.jpg",
            "userId": "116416151083804773706"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 100001\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  with tf.Session(graph=graph, config=config) as session:\n",
        "      tf.global_variables_initializer().run()\n",
        "      print('Initialized')\n",
        "      average_loss = 0\n",
        "\n",
        "      for step in range(num_steps):\n",
        "          batch_data, batch_labels = generate_batch_cow(batch_size, cbow_r)\n",
        "\n",
        "          feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
        "\n",
        "          _, l = session.run([train_op, loss], feed_dict=feed_dict)\n",
        "\n",
        "          average_loss += l\n",
        "          if step % 2000 == 0:\n",
        "              if step > 0:\n",
        "                  average_loss = average_loss / 2000\n",
        "              # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "              print('Average loss at step %d: %f' % (step, average_loss))\n",
        "              average_loss = 0\n",
        "\n",
        "          # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "          if step % 10000 == 0:\n",
        "              pred = predictions.eval()\n",
        "              for j in range(0,valid_examples.shape[0]):\n",
        "                  print('    valid_examples ',j,': ', [reverse_dictionary[bi] for bi in valid_examples[:][j]], \n",
        "                        ':---------',reverse_dictionary[np.argmax(pred[j])],';------:', reverse_dictionary[valid_labels[j]])\n",
        "      final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 0: 8.722609\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- insurence ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- infamia ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- penalise ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- susitna ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- nicety ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- strawpedo ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- sordid ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- transferase ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- rapoport ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- chevalines ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- buffy ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- cristino ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- berlin ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- hibernians ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- concedes ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- seized ;------: that\n",
            "Average loss at step 2000: 5.934533\n",
            "Average loss at step 4000: 4.178922\n",
            "Average loss at step 6000: 3.759257\n",
            "Average loss at step 8000: 3.521706\n",
            "Average loss at step 10000: 3.441034\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- wealth ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- as ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- the ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- the ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- the ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- moon ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- of ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- many ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- to ;------: that\n",
            "Average loss at step 12000: 3.452378\n",
            "Average loss at step 14000: 3.404186\n",
            "Average loss at step 16000: 3.394294\n",
            "Average loss at step 18000: 3.365813\n",
            "Average loss at step 20000: 3.218143\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- the ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- base ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- the ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- is ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- comes ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- is ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- the ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- to ;------: that\n",
            "Average loss at step 22000: 3.277348\n",
            "Average loss at step 24000: 3.239868\n",
            "Average loss at step 26000: 3.223024\n",
            "Average loss at step 28000: 3.210568\n",
            "Average loss at step 30000: 3.174325\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- development ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- is ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- the ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- is ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- descended ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- is ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- the ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- to ;------: that\n",
            "Average loss at step 32000: 2.968654\n",
            "Average loss at step 34000: 3.160851\n",
            "Average loss at step 36000: 3.117699\n",
            "Average loss at step 38000: 3.123634\n",
            "Average loss at step 40000: 3.097870\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- the ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- is ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- the ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- has ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- ranging ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- is ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- many ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- to ;------: that\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step 42000: 3.124991\n",
            "Average loss at step 44000: 3.082338\n",
            "Average loss at step 46000: 3.070993\n",
            "Average loss at step 48000: 3.020359\n",
            "Average loss at step 50000: 3.004976\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- the ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- is ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- is ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- has ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- ranging ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- is ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- some ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- as ;------: that\n",
            "Average loss at step 52000: 3.042587\n",
            "Average loss at step 54000: 3.049253\n",
            "Average loss at step 56000: 2.876723\n",
            "Average loss at step 58000: 2.958149\n",
            "Average loss at step 60000: 2.983004\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- end ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- is ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- the ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- has ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- derives ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- in ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- some ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- as ;------: that\n",
            "Average loss at step 62000: 2.960239\n",
            "Average loss at step 64000: 2.893239\n",
            "Average loss at step 66000: 2.881900\n",
            "Average loss at step 68000: 2.902108\n",
            "Average loss at step 70000: 2.987164\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- the ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- or ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- the ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- has ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- descended ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- in ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- many ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- in ;------: that\n",
            "Average loss at step 72000: 2.949582\n",
            "Average loss at step 74000: 2.811747\n",
            "Average loss at step 76000: 2.937174\n",
            "Average loss at step 78000: 2.971043\n",
            "Average loss at step 80000: 2.851861\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- netherlands ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- caused ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- the ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- descended ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- is ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- several ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- the ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- as ;------: that\n",
            "Average loss at step 82000: 2.847803\n",
            "Average loss at step 84000: 2.838259\n",
            "Average loss at step 86000: 2.890452\n",
            "Average loss at step 88000: 2.905110\n",
            "Average loss at step 90000: 2.755713\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- use ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- caused ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- hebrew ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- in ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- descended ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- in ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- some ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- industrial ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- as ;------: that\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step 92000: 2.873050\n",
            "Average loss at step 94000: 2.839710\n",
            "Average loss at step 96000: 2.779775\n",
            "Average loss at step 98000: 2.441414\n",
            "Average loss at step 100000: 2.574801\n",
            "    valid_examples  0 :  ['and', 'the', 'culottes', 'of'] :--------- the ;------: sans\n",
            "    valid_examples  1 :  ['without', 'archons', 'chief', 'king'] :--------- the ;------: ruler\n",
            "    valid_examples  2 :  ['a', 'positive', 'by', 'self'] :--------- as ;------: label\n",
            "    valid_examples  3 :  ['abolished', 'although', 'are', 'differing'] :--------- they ;------: there\n",
            "    valid_examples  4 :  ['word', 'anarchism', 'derived', 'from'] :--------- hebrew ;------: is\n",
            "    valid_examples  5 :  ['first', 'used', 'early', 'working'] :--------- the ;------: against\n",
            "    valid_examples  6 :  ['has', 'also', 'taken', 'up'] :--------- been ;------: been\n",
            "    valid_examples  7 :  ['it', 'has', 'been', 'taken'] :--------- had ;------: also\n",
            "    valid_examples  8 :  ['from', 'the', 'without', 'archons'] :--------- time ;------: greek\n",
            "    valid_examples  9 :  ['against', 'early', 'class', 'radicals'] :--------- the ;------: working\n",
            "    valid_examples  10 :  ['anarchism', 'originated', 'a', 'term'] :--------- as ;------: as\n",
            "    valid_examples  11 :  ['there', 'are', 'interpretations', 'of'] :--------- many ;------: differing\n",
            "    valid_examples  12 :  ['culottes', 'of', 'french', 'revolution'] :--------- the ;------: the\n",
            "    valid_examples  13 :  ['revolution', 'and', 'sans', 'culottes'] :--------- industrial ;------: the\n",
            "    valid_examples  14 :  ['pejorative', 'way', 'describe', 'any'] :--------- to ;------: to\n",
            "    valid_examples  15 :  ['any', 'act', 'used', 'violent'] :--------- to ;------: that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QLf9xA6bmQFt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}