{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296970 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "a  hrtparyaoitesel aymopaecpvkihi k dsdzxlu ntsa qjgmsum f  rr guen haugmf dmel \n",
      "gdrhiexwlxrralitsdsl q r rgzaiderfeizhsjvwayyf vptpioma m r hznbo e yj ley t dnq\n",
      "n ofgpiugmrnhnlj c a n admlfct or ai bpflyujv rtcwndfqjohqp eoo jioorm nyehnqnod\n",
      "glpzrfxcaawcegbwzgczsvielint hsejno  munnmtdzpwh n vglehjasaxnjxwumwgd c zulando\n",
      "kwfu s ak sdpetz lzt mtq sqb z lmmyus owdexwfkbl zswbueaerptrd fazhnvewsiehwakeh\n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.586834 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.54\n",
      "Validation set perplexity: 10.45\n",
      "Average loss at step 200: 2.252792 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 2.090603 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.035138 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.976315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.897259 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 700: 1.872686 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.867995 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 900: 1.847112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 1000: 1.847500 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "or cerse dile for  aupporato aroud forvonty he danorign b one will nottred witen\n",
      "torary the smud cion of either agater majelah a laviding serie cm a lests ackmon\n",
      "ing termstroad an lodf and teinted to dusance the aseking for house one nine is \n",
      "hactions ancomedician knonch k one framsed m disude v n a suflic gay dickin laun\n",
      "d seden s gmervaniem med is as descain as degradic will clam owhiam actrotetw tw\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.798116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.771813 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1300: 1.758989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1400: 1.763480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1500: 1.748236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1600: 1.731591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.712579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1800: 1.690358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1900: 1.696519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.676557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "panews amenicals numitay procemstry ulares after the siff two zero feated uses i\n",
      "zer eidgand of manforage of wide a s these to by amerect dm and girlardent loig \n",
      "abes airmple notal the micopbire is the of he dames comples the lato lorrged cou\n",
      "ce one nine five zero zere ministly to noting siffer thoss instrod oply through \n",
      "ed by ama are is comptanation decentials plactic compets illybated souft fecess \n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.688106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2200: 1.707230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.705317 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2400: 1.682880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.689009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.669906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2700: 1.681767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2800: 1.683502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2900: 1.677031 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3000: 1.681475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "pement encortion of the pharom strater caute of refer to histody boew the siffer\n",
      "gen a pknder cadel of allter usurom their of antrepide to brimbine sernity prote\n",
      "late a were nations merpused about to the amptat one nine one six nolation in tw\n",
      "ovel ifderure borotions condom issuezial fired in on the outen a crap refio the \n",
      "cloge to caleg they metable jorus for extliage prokel after meni it of the sturp\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.651393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.642332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3300: 1.649146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3400: 1.637913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.676413 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3600: 1.650409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3700: 1.651749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3800: 1.656156 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.651506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4000: 1.642199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "hew sidepaim ishiptia west while photuce the undamathyra gained stres of the dor\n",
      "jusing of ad instandon bejorme in shipple both is aution for histeralle atams vi\n",
      "mes nempectsing conquine of sector deoler ga bee with one nine eight eight six k\n",
      "kiers but transport with a castiration blighe a idep one zero geope one nine sev\n",
      "pility with with is sivers but sacoption vlykes that aft more of lond asseat oft\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4100: 1.616821 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4200: 1.615803 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4300: 1.617775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4400: 1.608533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.639183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4600: 1.620677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4700: 1.623162 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4800: 1.610071 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4900: 1.616233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.612358 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "el the emperorments states the complience writee the quings nerbin these the fou\n",
      "undes gear woold from gunded lloy because of hay water actors all opets prosceut\n",
      "uls preselsform the dell pon to among thignic the killy kiscoprone s tour dircon\n",
      "ce during the present unspeass the two zero liry of one nine four now at geold c\n",
      "net three the notcos beewn book eight i world indems coussop is especionific it \n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5100: 1.592986 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5200: 1.593612 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.593825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.590580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5500: 1.587998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.561476 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.581390 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.602139 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5900: 1.578544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6000: 1.581575 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "s cold it is ie drost the of the namen show lated that the consublats the consti\n",
      "g samism efficed many however airing and say awe or proke prokic for have grains\n",
      "vences and charge eiffel difbenbation for the ofference in the maincablied knoth\n",
      "al mickde lack occlugac were androrter well on the was recroyress and resedve ha\n",
      "bogify not was is querle sutopers floging grow three note of order vijaid tech e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6100: 1.574469 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6200: 1.585457 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6300: 1.581187 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.572190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6500: 1.556499 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6600: 1.599388 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6700: 1.574457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6800: 1.575389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6900: 1.571766 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7000: 1.590346 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "y the being the was niverphell tribet was plays decreatal to often air ment bang\n",
      " gas the bavord nout final methanese not trainence al often textem speera dun th\n",
      "d homen made seever the tembtate echup diejanting recognumarb or giod becomentin\n",
      "cyply he syrt sea iwiticing counts hylobrarseves dettengine royah chrietwadi the\n",
      "x or of even writers as deslevy a now swayul of exclminb sermending that german \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7100: 1.592289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7200: 1.569935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7300: 1.552663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7400: 1.560217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7500: 1.602359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7600: 1.570832 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7700: 1.577857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7800: 1.593104 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7900: 1.608742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 8000: 1.540161 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "================================================================================\n",
      "y beage caniss system resigner also two citor bardan doothuls eximed hewhiltzy v\n",
      "y and new yould had not clest pry four ie the progress the origins datocarian ll\n",
      "f the rcl one three b vased autories and is in retoos reanight a recemved are ma\n",
      "d monime in hearled and stutued as for heb like gae revinge and to world includi\n",
      "jest is longuityer who limed one nine nine five jail you anithed that has sen fa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 8100: 1.535460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 8200: 1.583742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 8300: 1.565914 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 8400: 1.559482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 8500: 1.550258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 8600: 1.568022 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 8700: 1.597019 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 8800: 1.565207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8900: 1.575487 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 9000: 1.593165 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "vest of mys and other by men islands oruas of the miractarping in ido aids ages \n",
      " to one nine eight nine four chilgaf many and dord cridipanines umager by fore f\n",
      "y technoged to bnek micromorging heaprieic agencuated in one one eight goth to f\n",
      "mans to and to the nearny in the buranizaters sobel s seried republically astrua\n",
      "ver of linets with not the because spact would desided to aguit no of q one memi\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.50\n",
      "Average loss at step 9100: 1.590425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 9200: 1.558766 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 9300: 1.574378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 9400: 1.574767 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 9500: 1.613408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 9600: 1.585156 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 9700: 1.601259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 9800: 1.566477 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 9900: 1.573005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 10000: 1.578815 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "pain xotepheh in nastem aprist of international gildan comporth in limital power\n",
      "geration sentishation monumes is and rife the mook depacker is amons day dame su\n",
      "minatiby in one main the lenger are public and roumal generally this fliting enc\n",
      "ments whicher jewabage the four tralites britilistal studen moon recordident of \n",
      " malded cana bass of the teits in the central is ovs six eight two eight three t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 10100: 1.581113 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 10200: 1.578994 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10300: 1.569915 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 10400: 1.568467 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10500: 1.566874 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10600: 1.576157 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10700: 1.578052 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10800: 1.575359 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 10900: 1.567971 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11000: 1.576113 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "le minise accear of much one eight eight six one other to f amerists but works r\n",
      "s with two williamogi isskentism that at timman sion activities by electroluedic\n",
      "an inexommes to agenal enciention dy rice and day wets make to into emplay shood\n",
      "z memorilica public for same construdly mensices spart the exchore in has dess d\n",
      "y as leonish according flous include anologisa med and the first farmers it heop\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11100: 1.578490 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11200: 1.566027 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11300: 1.552216 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11400: 1.585180 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11500: 1.594673 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 11600: 1.599108 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11700: 1.573261 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11800: 1.562080 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11900: 1.576218 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 12000: 1.597627 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "s ecto on three favish theme from time is humamost litts of centomy f one eight \n",
      "ullar increased to n international one moted that the is interam was still minip\n",
      "d ibble two thouskers the tew in none wikey of among bash jaisty as four zero ze\n",
      "to one numbert fiction etiation of angtardan is many constraimed or olided aller\n",
      "g relations unsorved over new first of the pasic of the tuper jame nestages hely\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 12100: 1.628865 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 12200: 1.623361 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 12300: 1.586795 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 12400: 1.587345 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 12500: 1.575495 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 12600: 1.607215 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 12700: 1.577349 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 12800: 1.568603 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 12900: 1.588325 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13000: 1.555854 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "ther solute to that alternal its mudus all catlent centirily wnrickal boyt klama\n",
      "ke of the more confeder of letvication and formal texts of they fediry king a pl\n",
      "brair to and s gultures and the brim ruge sparensty german ischarging for in one\n",
      "hicteary these ever white of guing mart of prilg of or gas technical proses spec\n",
      "cess on the funally of quillus woulderstoe other relia bill d four duch as other\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13100: 1.584941 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13200: 1.599577 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13300: 1.600326 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13400: 1.638534 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13500: 1.629736 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13600: 1.598428 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13700: 1.593163 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13800: 1.577891 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 13900: 1.563754 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14000: 1.579056 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "d ramay greating that centershen grewked an csix other similisain developed whic\n",
      "heighitor however interfically firsts divis system in which opposoted vlozs smal\n",
      "ch of misport to with the was hiburlose marn blua three recently are what with e\n",
      "k to unioms one nine three zero one five five five nine and and all three essign\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weft inlo on one five signish five the country de germanon one nine eight zero n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14100: 1.551331 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14200: 1.590883 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14300: 1.593124 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 14400: 1.573203 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 14500: 1.561981 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14600: 1.581348 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 14700: 1.573741 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 14800: 1.582706 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 14900: 1.581723 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 15000: 1.559883 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "ing fartic rosed activisway hell fifliated muricol supse formed vigeed to one ni\n",
      "ing peasenan prims the forst villext metal as other strastions for actain bisepp\n",
      "howe of larved stited from the chinews the nawardwaldirity with all system malli\n",
      "reevers english chirgny recording one nine nine and the eight supporte earcdan a\n",
      "reinal the city s jossephisms cuchinal wings of the minius in the repress was ch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 15100: 1.568532 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 15200: 1.581380 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15300: 1.586150 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15400: 1.574736 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15500: 1.572120 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15600: 1.559227 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15700: 1.567206 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15800: 1.598578 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15900: 1.581820 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16000: 1.590216 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "mys war the custalized of not numbers and as in pare if distripteool whie undema\n",
      "ration a move the periods g the english pronecial one five zero two km of movil \n",
      "ver he order right dome histoo algec on orthologow righles for a d one six zero \n",
      "ann art it is umison by libited between were parrial of k mose of a politicorial\n",
      "rela fighter s faitok cource war airdsto is and a commere kingologhaching small \n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16100: 1.599997 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16200: 1.607414 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16300: 1.597836 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16400: 1.576836 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 16500: 1.599459 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 16600: 1.591419 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 16700: 1.587457 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 16800: 1.589990 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 16900: 1.592442 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17000: 1.612518 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "x its anay becal was hongerire noman process and the nine and old a reminintance\n",
      "mora diminanle in cread a boodul effect counts attecepnking immigific mekion wen\n",
      "ked cann of jupinim spaces of x cyssexbors in what in earth one of the battliago\n",
      "jement film compates chegire of ellows state an a free some rangeslaft all obsid\n",
      "ricy state n evided fron over nellerite nukese s calt author reportably weabre f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17100: 1.580300 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17200: 1.590820 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17300: 1.584882 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17400: 1.581137 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17500: 1.572017 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17600: 1.588568 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17700: 1.563602 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17800: 1.560384 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 17900: 1.560079 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18000: 1.591003 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "d of coneing region a significantedters glay to gas both of sulprau about in the\n",
      "ound gammourd treatioestencien and buricho such and lither self to party formal \n",
      "tei stabline of welk but applement homen two any to the igsing into lighteres th\n",
      "wezarly diskith order but foust surfased higher fars thounding scyticla theory p\n",
      "real afra these the famous in officestate of theres french senabing significant \n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18100: 1.592522 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18200: 1.583444 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18300: 1.547584 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18400: 1.565141 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18500: 1.562105 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18600: 1.556885 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18700: 1.568796 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18800: 1.580518 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 18900: 1.608280 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19000: 1.591664 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "ust favisuar need in the she means throuftor espectry copsencieth the comm five \n",
      "g stores freeck was electron were chancer and upology as f mestary s reitary the\n",
      "porate is a digital kay over formivalaf to of the corporater ground out it is vi\n",
      "ationed births and econolly painca to article chept the virging lapornius four t\n",
      "ads person wested other time yoth foeld reliberat consearaticss woild forceen of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19100: 1.574664 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19200: 1.595731 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19300: 1.568504 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19400: 1.563656 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19500: 1.560509 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19600: 1.553953 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19700: 1.589521 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19800: 1.577836 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 19900: 1.587138 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20000: 1.550231 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "que which lozwip the five three three sed machetherisort houres laping spaating \n",
      "our the poort deviveon jew antgram repeeso the tour maresed quysenge clustly the\n",
      "ger one nine one six one beoffint while as the soukin two and carked nor girsmio\n",
      "x ported moitt of the suecable man large and that the churg user with an decend \n",
      "ginicallightaran capital formating compeant to some imports and acone is laged c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20100: 1.591704 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20200: 1.596165 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20300: 1.576331 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20400: 1.591303 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20500: 1.601167 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20600: 1.578640 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20700: 1.577328 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20800: 1.589894 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 20900: 1.623736 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21000: 1.609156 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "y occumite a chilons age groupt inhisus military afver a rade to daxinated commo\n",
      "ctor which person wooks mart deci is prea has in one five zerona and the ailand \n",
      "y heading interes into panislin remublia public of the kamena a marese the micdo\n",
      "jemenh prides are in pay of continue only of fultioned such as abiced in the fli\n",
      "ze cood was the finds of to eway is the million by casa in can unitedil chrise o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21100: 1.590363 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21200: 1.565782 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21300: 1.577477 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21400: 1.588473 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21500: 1.590421 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21600: 1.571533 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21700: 1.584426 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21800: 1.569312 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 21900: 1.588755 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22000: 1.588580 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "riguest even head coners who hiticiany at the subsidestions theoxity hohow was r\n",
      "s plays outvidiwas a mows of has peaniscy results deeffacisture the ineling clar\n",
      "jokethers to ru decomencies year populareption and the dro langual stybil out at\n",
      "wight calenio and buel danged gin are anymour alternation of the population or b\n",
      "ation dcple to despitirat ltiger unespirn spimic dobbot then nol the imitted str\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22100: 1.576452 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22200: 1.572877 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22300: 1.566397 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22400: 1.575058 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22500: 1.596140 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22600: 1.590446 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22700: 1.573103 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22800: 1.610205 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 22900: 1.613023 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23000: 1.571519 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "quak that partaates fulther have a other as a belinge he level on spetyered on o\n",
      "quate eight six two zero zero zero four two one five so one nine two nine five s\n",
      "sched playny subcitors astuge in medbe youble accury of from seven hoodinia rach\n",
      " of the trair i wile of the leaguesta to the unitimieal the varive pall infusati\n",
      "y dancey slate sequence of ends for hoyo the hower c particulay the democratic f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23100: 1.573231 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23200: 1.563679 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23300: 1.603482 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23400: 1.594249 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23500: 1.564230 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23600: 1.560938 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23700: 1.567485 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23800: 1.587378 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 23900: 1.551247 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24000: 1.552894 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "je goot thiss barts a one nine seven eight eight zero novel every of ans masses \n",
      " changes opsidings of mathered set yones of hhight seven two the hilvers three s\n",
      "har with ashi anvarril a merged and recenture image let uscussiant framer socibu\n",
      "as bioldual upsec arbic carkk cause and seven it belated by bew catabuller detan\n",
      "f goods or problencing into emplic sofficiented clo avyecements and on anover ar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24100: 1.588994 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24200: 1.594286 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24300: 1.584069 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24400: 1.578367 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24500: 1.566726 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24600: 1.542899 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24700: 1.568791 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24800: 1.577609 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 24900: 1.603177 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25000: 1.573134 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "d back and of the yeression known archetion are the let in the new ff track head\n",
      "condonists quiek four in evolutive and onlyss by a partification descrite emplio\n",
      "g in same callicals invoment lintour voilenticy and ispanfi from the they is the\n",
      "que were depqrications emprosce the primfided of the place of or methaye and rel\n",
      "g and desidy and  attemparish because the ound lotbering mijopecule tagm commone\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25100: 1.592593 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25200: 1.567034 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25300: 1.589323 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25400: 1.579610 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25500: 1.575233 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25600: 1.566131 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25700: 1.575670 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25800: 1.587261 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 25900: 1.573057 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26000: 1.590539 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "hen involves the marists prost is alquatoring fiberaft thus index wak balk aguua\n",
      "que schor empopulable consistingment are accounty of polutation develops five st\n",
      "ab mate storied his hiftenism and the semantx then interact thonophyin to a the \n",
      "fuction of boqys dirique gothroman cdil the planing from dellans ulled is whethe\n",
      "phipatiby center in dalled beonson disconony the tes gines hall features also re\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26100: 1.595337 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26200: 1.574520 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26300: 1.589714 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26400: 1.583690 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26500: 1.573039 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26600: 1.586366 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26700: 1.571044 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26800: 1.559983 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 26900: 1.588639 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27000: 1.578101 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "undast novid the compoint that may result musical often wowergce the flistop whe\n",
      "zer r pome phetasicals wabisoff mementies film mewho vynds the has unit three cl\n",
      "rabile eup reseaple society their veryio in the dapes youm subsfyssel prograpicl\n",
      "kei who ga dehiates cuntia later uses identary some livers from ecoveres by leco\n",
      "wobility methol is is cara yevilg the reprodution the historic ebroppements stal\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27100: 1.608323 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27200: 1.577869 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27300: 1.589036 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27400: 1.605960 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27500: 1.629109 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27600: 1.602082 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27700: 1.602513 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27800: 1.574582 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 27900: 1.556031 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28000: 1.586266 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "oves of enside chrown on thrighwand that from two his orghod the purded excienth\n",
      "ker be condi also origincer some there at for ng mediably attansion of the niqus\n",
      "bect in numarels lither hid tetto doys of hilllame wsilure such quay arrialisc o\n",
      "rea slows the were co m workarlia bery bs american with was a sown from that yel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y rea cheach posile and usely have featurism canthoue potermored mild eftermatio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28100: 1.583856 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28200: 1.584652 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28300: 1.593890 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28400: 1.593187 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28500: 1.586549 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28600: 1.562292 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28700: 1.592400 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28800: 1.598572 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 28900: 1.594863 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29000: 1.590623 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "ca with the halks repre about hope of the difers one nine six also knemes symone\n",
      "wite obbers of the lea to gave of and founded bu beingjon those the origital at \n",
      " aderitellound consists bright specient indider a rulven one cwandous by the mon\n",
      "ing as chaysia to engly viele lifelogians the to f most whal in edreguethern ede\n",
      "ften another partinicable as influencen of mendriag imgotare the acclerates the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29100: 1.573166 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29200: 1.571975 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29300: 1.595697 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29400: 1.592764 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29500: 1.605940 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29600: 1.601591 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29700: 1.602578 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29800: 1.604896 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 29900: 1.587433 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30000: 1.597916 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "e for afriga evabstroupple hend dewended in ond by harlage aucters identifies a \n",
      "x technited judabes in brothett have role to extinction and argumate as were cha\n",
      " canse of al lemamists and merreit and a vina divisuentia in by moiny amout they\n",
      "s a formers lafte contravies their jonk cavauliabilical led subjects heny when p\n",
      "x is libable idocys under parac respream sopien cordogry coool is his coasshed a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30100: 1.611050 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30200: 1.628793 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30300: 1.609541 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30400: 1.591147 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30500: 1.607825 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30600: 1.605151 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30700: 1.611285 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30800: 1.613485 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 30900: 1.582850 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31000: 1.602020 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "mox of one nine nine one is the first exators golds bleen versected by full arch\n",
      "warhsian ear to leading of baved followed the grow propolities one nine nine fiv\n",
      "fious badere from home institlory acrobing the be bagend for allawa one zero zer\n",
      "tul the loces files one nine eight to as wester christern strendind slave of out\n",
      "ish antiletitiam sugm gartet pistice iudopu mathanise surels of ant in soling ex\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31100: 1.616900 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31200: 1.599037 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31300: 1.587793 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31400: 1.606959 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31500: 1.623795 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31600: 1.559683 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31700: 1.572476 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31800: 1.590520 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 31900: 1.604045 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32000: 1.572344 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "y however housess of starding propope brince incar that humandarn to common tand\n",
      "d the properial usear of many gengus weoth noturgams setis and be coner to suckl\n",
      "wit five sever three two one one one nine three five the dilltwed and is the mal\n",
      "gs grown and bouthhon itbrids and indust an ever of the use websing to bulshsion\n",
      "joosb in in the tinable of ease memical tay one nine seven zero hame leading ims\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32100: 1.566919 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32200: 1.562899 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32300: 1.557359 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32400: 1.573068 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32500: 1.561589 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32600: 1.539892 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32700: 1.528287 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 32800: 1.559123 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 32900: 1.551670 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33000: 1.533833 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "ulker telefices bunnest aldin coterseld the discrituals there solum out ogerar s\n",
      "s paterfilles the being north were locdon they abilitiby support of ghungs or se\n",
      "er reverianity for the leensic colocias to prepensible to the sarah such not ort\n",
      "et esmall goid bereing actually island other and efbers uniblleache peigin s in \n",
      "ret currem of lagralite of eat third a discrivative stracked in starmons me hist\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33100: 1.537226 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33200: 1.561987 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33300: 1.556496 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33400: 1.593316 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33500: 1.557416 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33600: 1.545474 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33700: 1.558468 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33800: 1.571489 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 33900: 1.593630 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34000: 1.558485 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "lics gutwine northerity being arry assurard buel him by surible abbids signium a\n",
      "uther many spartiess exharfan of autovetiv saint libiticide mafter chill of abou\n",
      "zen english order have the polyut cluse eight explorically hod extrational hen w\n",
      "prives earler be peacters funcy of the providual of furtues was of the dema word\n",
      "a obnered playas and taping a shint exinned in mino to the largy in the sead art\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34100: 1.568487 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34200: 1.603984 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34300: 1.586134 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34400: 1.596763 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34500: 1.598943 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34600: 1.590048 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34700: 1.555047 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34800: 1.539161 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 34900: 1.561071 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35000: 1.575061 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "views three four allowes ons temphary definativelan s mathery of any desarments \n",
      "x razective in annited one three yite one nine eight three fove one that goodwar\n",
      "x than old indvoky exvicus ahelogrout apprenting which centration five five six \n",
      "diting seasonales hell emi schuse s maching called to ancounce they of the pnyid\n",
      "x the knightopy p hipten annotaed every which when anny this olffitestic metille\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35100: 1.580230 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35200: 1.581570 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35300: 1.571677 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35400: 1.583734 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35500: 1.612872 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35600: 1.585350 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35700: 1.604469 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35800: 1.592584 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 35900: 1.588703 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36000: 1.584148 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "many made dusting in they josblingsamo desultion allt markestaricas an and prozi\n",
      "y s fla sought in one eon mt was paters nnut need was is and one nine three m s \n",
      " one nine rohanists cardans lisciels the year agging one or four five aganthery \n",
      " nose host at line impusiture of first fart start by three duch any beene slorts\n",
      "wates philowand devaly frum nearle loster bewnan with a loneugraly the further t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36100: 1.564494 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36200: 1.588497 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36300: 1.560532 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36400: 1.569867 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36500: 1.524086 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36600: 1.551074 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36700: 1.535490 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36800: 1.529486 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 36900: 1.549234 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37000: 1.557532 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "f thomging in light conceat differentliniminal are two eight t near f most altai\n",
      "ke hes rus george chandsionous is kean in a senting say mades to intellists of t\n",
      "ubh general vowe her impolyus roako as two divishey been in dessinsing such forc\n",
      "boll furded which in quild orsfiencation podere two seven yerm to slaving four t\n",
      "way hubhee spendrially housed programming line racion opees in the moinali who c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37100: 1.562172 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37200: 1.532512 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37300: 1.517268 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37400: 1.552707 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37500: 1.574445 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37600: 1.559969 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37700: 1.583363 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37800: 1.543855 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 37900: 1.559995 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38000: 1.569379 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "one a f thus daygi camorary this exectebys over a setwerbing throote zeronz gen \n",
      "menor brim that a brees to releggnoging comedus in free the decalto syxtowial te\n",
      "d dured s set at was in aval be verkany mi and godd one goods it is reagenage th\n",
      "s a vas a but sing mathund was explace and time primately information of the dec\n",
      "th includest extemicath so ret less other roughtions by or two dake hand a becam\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38100: 1.557467 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38200: 1.585279 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38300: 1.596554 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38400: 1.631135 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38500: 1.614706 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38600: 1.635390 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38700: 1.618659 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38800: 1.597689 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 38900: 1.598093 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39000: 1.565117 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "urus and its sourcents are also d one five zero another two zero zero one three \n",
      "ques and others out on upade inye desimiting have pach pplim two one eight seven\n",
      "x the not has as was the life of slo work the forton rick carry one nine five le\n",
      "nique doced from the its stated the irelests rolesia gable ruighoushery and choo\n",
      "king englogtion depatent on the largengation it was schorichent in knowleds nual\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39100: 1.561233 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39200: 1.586140 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39300: 1.582575 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39400: 1.619313 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39500: 1.606618 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39600: 1.619158 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39700: 1.609660 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39800: 1.624460 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 39900: 1.597063 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40000: 1.657123 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "x highing in duslatis by imper his indeamners recemps addytion a used in the poi\n",
      "ze roov ensimic iis j recopedes mina kladency spopular was relativ sequent fames\n",
      "ver than during the remucistied agorise fits vien or to auchiamouril his marance\n",
      "zery alamons o gigine is orcry restitution the bass otherial phoory in conial ph\n",
      "ing musical romby diselverosmers chinabut with aiclor by senjulal polements empl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40100: 1.626654 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40200: 1.603272 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40300: 1.611553 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40400: 1.594218 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40500: 1.588435 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40600: 1.594441 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40700: 1.603815 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 40800: 1.591362 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ce50ea4a9135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmean_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unrollings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-64cd68020dff>\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-64cd68020dff>\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 70001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "jupyter3_Python_3",
   "language": "python",
   "name": "jupyter3_python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
